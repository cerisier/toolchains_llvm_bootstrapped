--- a/third_party/intel/clang/tmmintrin.h	2025-12-31 15:03:19
+++ b/third_party/intel/clang/tmmintrin.h	2025-12-31 15:05:11
@@ -37,11 +37,13 @@
 ///    A 64-bit vector of [8 x i8].
 /// \returns A 64-bit integer vector containing the absolute values of the
 ///    elements in the operand.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_abs_pi8(__m64 __a)
 {
     return (__m64)__builtin_ia32_pabsb((__v8qi)__a);
 }
+#endif
 
 /// Computes the absolute value of each of the packed 8-bit signed
 ///    integers in the source operand and stores the 8-bit unsigned integer
@@ -73,11 +75,13 @@
 ///    A 64-bit vector of [4 x i16].
 /// \returns A 64-bit integer vector containing the absolute values of the
 ///    elements in the operand.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_abs_pi16(__m64 __a)
 {
     return (__m64)__builtin_ia32_pabsw((__v4hi)__a);
 }
+#endif
 
 /// Computes the absolute value of each of the packed 16-bit signed
 ///    integers in the source operand and stores the 16-bit unsigned integer
@@ -109,11 +113,13 @@
 ///    A 64-bit vector of [2 x i32].
 /// \returns A 64-bit integer vector containing the absolute values of the
 ///    elements in the operand.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_abs_pi32(__m64 __a)
 {
     return (__m64)__builtin_ia32_pabsd((__v2si)__a);
 }
+#endif
 
 /// Computes the absolute value of each of the packed 32-bit signed
 ///    integers in the source operand and stores the 32-bit unsigned integer
@@ -176,8 +182,10 @@
 ///    An immediate operand specifying how many bytes to right-shift the result.
 /// \returns A 64-bit integer vector containing the concatenated right-shifted
 ///    value.
+#ifndef COSMO_DISABLE_MMX
 #define _mm_alignr_pi8(a, b, n) \
   ((__m64)__builtin_ia32_palignr((__v8qi)(__m64)(a), (__v8qi)(__m64)(b), (n)))
+#endif
 
 /// Horizontally adds the adjacent pairs of values contained in 2 packed
 ///    128-bit vectors of [8 x i16].
@@ -242,11 +250,13 @@
 ///    destination.
 /// \returns A 64-bit vector of [4 x i16] containing the horizontal sums of both
 ///    operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_hadd_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_phaddw((__v4hi)__a, (__v4hi)__b);
 }
+#endif
 
 /// Horizontally adds the adjacent pairs of values contained in 2 packed
 ///    64-bit vectors of [2 x i32].
@@ -265,11 +275,13 @@
 ///    destination.
 /// \returns A 64-bit vector of [2 x i32] containing the horizontal sums of both
 ///    operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_hadd_pi32(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_phaddd((__v2si)__a, (__v2si)__b);
 }
+#endif
 
 /// Horizontally adds, with saturation, the adjacent pairs of values contained
 ///    in two packed 128-bit vectors of [8 x i16].
@@ -317,11 +329,13 @@
 ///    destination.
 /// \returns A 64-bit vector of [4 x i16] containing the horizontal saturated
 ///    sums of both operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_hadds_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_phaddsw((__v4hi)__a, (__v4hi)__b);
 }
+#endif
 
 /// Horizontally subtracts the adjacent pairs of values contained in 2
 ///    packed 128-bit vectors of [8 x i16].
@@ -386,11 +400,13 @@
 ///    the destination.
 /// \returns A 64-bit vector of [4 x i16] containing the horizontal differences
 ///    of both operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_hsub_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_phsubw((__v4hi)__a, (__v4hi)__b);
 }
+#endif
 
 /// Horizontally subtracts the adjacent pairs of values contained in 2
 ///    packed 64-bit vectors of [2 x i32].
@@ -409,11 +425,13 @@
 ///    the destination.
 /// \returns A 64-bit vector of [2 x i32] containing the horizontal differences
 ///    of both operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_hsub_pi32(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_phsubd((__v2si)__a, (__v2si)__b);
 }
+#endif
 
 /// Horizontally subtracts, with saturation, the adjacent pairs of values
 ///    contained in two packed 128-bit vectors of [8 x i16].
@@ -461,11 +479,13 @@
 ///    the destination.
 /// \returns A 64-bit vector of [4 x i16] containing the horizontal saturated
 ///    differences of both operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_hsubs_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_phsubsw((__v4hi)__a, (__v4hi)__b);
 }
+#endif
 
 /// Multiplies corresponding pairs of packed 8-bit unsigned integer
 ///    values contained in the first source operand and packed 8-bit signed
@@ -525,11 +545,13 @@
 ///    \a R1 := (\a __a2 * \a __b2) + (\a __a3 * \a __b3) \n
 ///    \a R2 := (\a __a4 * \a __b4) + (\a __a5 * \a __b5) \n
 ///    \a R3 := (\a __a6 * \a __b6) + (\a __a7 * \a __b7)
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_maddubs_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_pmaddubsw((__v8qi)__a, (__v8qi)__b);
 }
+#endif
 
 /// Multiplies packed 16-bit signed integer values, truncates the 32-bit
 ///    products to the 18 most significant bits by right-shifting, rounds the
@@ -565,11 +587,13 @@
 ///    A 64-bit vector of [4 x i16] containing one of the source operands.
 /// \returns A 64-bit vector of [4 x i16] containing the rounded and scaled
 ///    products of both operands.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_mulhrs_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_pmulhrsw((__v4hi)__a, (__v4hi)__b);
 }
+#endif
 
 /// Copies the 8-bit integers from a 128-bit integer vector to the
 ///    destination or clears 8-bit values in the destination, as specified by
@@ -616,11 +640,13 @@
 ///    destination. \n
 ///    Bits [3:0] select the source byte to be copied.
 /// \returns A 64-bit integer vector containing the copied or cleared values.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_shuffle_pi8(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_pshufb((__v8qi)__a, (__v8qi)__b);
 }
+#endif
 
 /// For each 8-bit integer in the first source operand, perform one of
 ///    the following actions as specified by the second source operand.
@@ -720,11 +746,13 @@
 ///    A 64-bit integer vector containing control bytes corresponding to
 ///    positions in the destination.
 /// \returns A 64-bit integer vector containing the resultant values.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_sign_pi8(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_psignb((__v8qi)__a, (__v8qi)__b);
 }
+#endif
 
 /// For each 16-bit integer in the first source operand, perform one of
 ///    the following actions as specified by the second source operand.
@@ -746,11 +774,13 @@
 ///    A 64-bit integer vector containing control words corresponding to
 ///    positions in the destination.
 /// \returns A 64-bit integer vector containing the resultant values.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_sign_pi16(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_psignw((__v4hi)__a, (__v4hi)__b);
 }
+#endif
 
 /// For each 32-bit integer in the first source operand, perform one of
 ///    the following actions as specified by the second source operand.
@@ -772,11 +802,13 @@
 ///    A 64-bit integer vector containing two control doublewords corresponding
 ///    to positions in the destination.
 /// \returns A 64-bit integer vector containing the resultant values.
+#ifndef COSMO_DISABLE_MMX
 static __inline__ __m64 __DEFAULT_FN_ATTRS_MMX
 _mm_sign_pi32(__m64 __a, __m64 __b)
 {
     return (__m64)__builtin_ia32_psignd((__v2si)__a, (__v2si)__b);
 }
+#endif
 
 #undef __DEFAULT_FN_ATTRS
 #undef __DEFAULT_FN_ATTRS_MMX
